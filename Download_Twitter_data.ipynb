{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Download Twitter data.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN7u/cYLFSTv3l/3CWEA407",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KavehKadkhoda/Social-Media-Data-Analytics/blob/main/Download_Twitter_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URFS36oPLPy3"
      },
      "outputs": [],
      "source": [
        "# Create a simple Python script to collect data from Twitter\n",
        "# how to use API (application programming interface) to collect data\n",
        "\n",
        "# What We Will Need\n",
        "# Authorization keys from Twitter\n",
        "# tweepy package for Python\n",
        "\n",
        "# Twitter provides developers with APIs and related documents in\n",
        "# https://dev.twitter.com/overview/documentation\n",
        "\n",
        "# This code for Twitter search will not run with the Essential access to Twitter (it gives 403 and 401 errors).\n",
        "# I applied for the academic research access in the Twitter Developer Portal, and with this access, I was able to run the code quickly!\n",
        "\n",
        "\n",
        "\n",
        "# To access and collect Twitter data, you need to have appropriate authentication\n",
        "# for the purpose of an application and/or a script:\n",
        "# Consumer key\n",
        "# Consumer secret\n",
        "# Access token\n",
        "# Access token secret\n",
        "\n",
        "\n",
        "# You can generate authentication information with creating an app for Twitter in\n",
        "# https://apps.twitter.com\n",
        "\n",
        "# Taken from by Social Media Data Analytics by Rutgers the State University of New Jersey @ Coursera\n",
        "\n",
        "\n",
        "# NOTE: THIS SCRIPT REQUIRES A FILE CALLED \"auth.k\" which contains (in this order):\n",
        "# CONSUMER_KEY\n",
        "# CONSUMER_SECRET\n",
        "# ACCESS_TOKEN\n",
        "# ACCESS_TOKEN_SECRET\n",
        "\n",
        "# App Key === API Key === Consumer API Key === Consumer Key === Customer Key === oauth_consumer_key\n",
        "# Callback URL === oauth_callback\n",
        "# Access token === Token === resulting oauth_token"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries\n",
        "import argparse\n",
        "from urllib.parse import urlparse \n",
        "import urllib\n",
        "import csv\n",
        "import tweepy"
      ],
      "metadata": {
        "id": "zCLr7gBKk4lD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# URL Cleanup\n",
        "\n",
        "def url_fix(s, charset='utf-8'):\n",
        "    if isinstance(s, unicode):\n",
        "        s = s.encode(charset, 'ignore')\n",
        "    scheme, netloc, path, qs, anchor = urlparse.urlsplit(s)\n",
        "    path = urllib.quote(path, '/%')\n",
        "    qs = urllib.quote_plus(qs, ':&=')\n",
        "    return urlparse.urlunsplit((scheme, netloc, path, qs, anchor))"
      ],
      "metadata": {
        "id": "T8IsAZi3k_EM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Command Parser\n",
        "\n",
        "def tw_parser(query, location, language, tweet_type, tweet_count):\n",
        "    global qw, ge, l, t, c, d\n",
        "\n",
        "# USE EXAMPLES:\n",
        "# =-=-=-=-=-=-=\n",
        "# % twsearch <search term>            --- searches term\n",
        "# % twsearch <search term> -g sf      --- searches term in SF geographic box <DEFAULT = none>\n",
        "# % twsearch <search term> -l en      --- searches term with lang=en (English) <DEFAULT = en>\n",
        "# % twsearch <search term> -t {m,r,p} --- searches term of type: mixed, recent, or popular <DEFAULT = recent>\n",
        "# % twsearch <search term> -c 12      --- searches term and returns 12 tweets (count=12) <DEFAULT = 1>\n",
        "# % twsearch <search term> -o {ca, tx, id, co, rtc)   --- searches term and sets output options <DEFAULT = ca, tx>\n",
        "\n",
        "# Parse the command\n",
        "    #parser = argparse.ArgumentParser(description='Twitter Search')\n",
        "    #parser.add_argument(action='store', dest='query', help='Search term string')\n",
        "    #parser.add_argument('-g', action='store', dest='loca', help='Location (lo, nyl, nym, nyu, dc, sf, nb')\n",
        "    #parser.add_argument('-l', action='store', dest='l', help='Language (en = English, fr = French, etc...)')\n",
        "    #parser.add_argument('-t', action='store', dest='t', help='Search type: mixed, recent, or popular')\n",
        "    #parser.add_argument('-c', action='store', dest='c', help='Tweet count (must be <50)')\n",
        "    #args = parser.parse_args()\n",
        "\n",
        "    qw = query     # Actual query word(s)\n",
        "    ge = ''\n",
        "\n",
        "    # Location\n",
        "    loca = location\n",
        "    if (not(loca in ('lo', 'nyl', 'nym', 'nyu', 'dc', 'sf', 'nb')) and (loca)):\n",
        "        print (\"WARNING: Location must be one of these: lo, nyl, nym, nyu, dc, sf, nb\")\n",
        "        #exit()\n",
        "    if loca:\n",
        "        ge = locords[loca]\n",
        "\n",
        "    # Language\n",
        "    l = language\n",
        "    if (not l):\n",
        "        l = \"en\"\n",
        "    if (not(l in ('en','fr','es','po','ko', 'ar'))):\n",
        "        print (\"WARNING: Languages currently supported are: en (English), fr (French), es (Spanish), po (Portuguese), ko (Korean), ar (Arabic)\")\n",
        "        #exit()\n",
        "\n",
        "    # Tweet type\n",
        "    t = tweet_type\n",
        "    if (not t):\n",
        "        t = \"recent\"\n",
        "    if (not(t in ('mixed','recent','popular'))):\n",
        "        print (\"WARNING: Search type must be one of: (m)ixed, (r)ecent, or (p)opular\")\n",
        "        #exit()\n",
        "\n",
        "    # Tweet count\n",
        "    if tweet_count:\n",
        "        c = int(tweet_count)\n",
        "        if (c > cmax):\n",
        "            print (\"Resetting count to \",cmax,\" (maximum allowed)\")\n",
        "            c = cmax\n",
        "        if (not (c) or (c < 1)):\n",
        "            c = 1\n",
        "    if not(c):\n",
        "        c = 1\n",
        "\n",
        "    print (\"Query: %s, Location: %s, Language: %s, Search type: %s, Count: %s\" %(qw,ge,l,t,c))"
      ],
      "metadata": {
        "id": "vyOQjHBOlFyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Authentication function\n",
        "\n",
        "def tw_oauth(authfile):\n",
        "    with open(authfile, \"r\") as f:\n",
        "        ak = f.readlines()\n",
        "    f.close()\n",
        "    auth1 = tweepy.auth.OAuthHandler(ak[0].replace(\"\\n\",\"\"), ak[1].replace(\"\\n\",\"\"))\n",
        "    auth1.set_access_token(ak[2].replace(\"\\n\",\"\"), ak[3].replace(\"\\n\",\"\"))\n",
        "    return tweepy.API(auth1)"
      ],
      "metadata": {
        "id": "fz_CXu_qlnBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tweepy search function\n",
        "\n",
        "def tw_search_json(query, cnt=5):\n",
        "    authfile = './auth.k'\n",
        "    api = tw_oauth(authfile)\n",
        "    results = {}\n",
        "    meta = {\n",
        "        'username': 'text',\n",
        "        'usersince': 'date',\n",
        "        'followers': 'numeric',\n",
        "        'friends': 'numeric',\n",
        "        'authorid': 'text',\n",
        "        'authorloc': 'geo',\n",
        "        'geoenable': 'boolean',\n",
        "        'source': 'text'\n",
        "    }\n",
        "    data = []\n",
        "    for tweet in tweepy.Cursor(api.search, q=query, count=cnt).items():\n",
        "        dTwt = {}\n",
        "        dTwt['username'] = tweet.author.name\n",
        "        dTwt['usersince'] = tweet.author.created_at      #author/user profile creation date\n",
        "        dTwt['followers'] = tweet.author.followers_count #number of author/user followers (inlink)\n",
        "        dTwt['friends']   = tweet.author.friends_count   #number of author/user friends (outlink)\n",
        "        dTwt['authorid']  = tweet.author.id              #author/user ID#\n",
        "        dTwt['authorloc'] = tweet.author.location        #author/user location\n",
        "        dTwt['geoenable'] = tweet.author.geo_enabled     #is author/user account geo enabled?\n",
        "        dTwt['source']    = tweet.source                 #platform source for tweet\n",
        "        data.append(dTwt)\n",
        "    results['meta'] = meta\n",
        "    results['data'] = data\n",
        "    return results\n",
        "\n",
        "def tw_search(api):\n",
        "    counter = 0\n",
        "    # Open/Create a file to append data\n",
        "    csvFile = open('twitter_result.csv','w')\n",
        "    #Use csv Writer\n",
        "    csvWriter = csv.writer(csvFile)\n",
        "    csvWriter.writerow([\"created\", \"text\", \"retwc\", \"hashtag\", \"followers\", \"friends\"])\n",
        "\t\n",
        "    for tweet in tweepy.Cursor(api.search,\n",
        "                                q = qw,\n",
        "                                g = ge,\n",
        "                                lang = l,\n",
        "                                result_type = t,\n",
        "                                count = c).items():\n",
        "\n",
        "        #TWEET INFO\n",
        "        created = tweet.created_at   #tweet created\n",
        "        text    = tweet.text         #tweet text\n",
        "        tweet_id = tweet.id          #tweet ID# (not author ID#)\n",
        "        cords   = tweet.coordinates  #geographic co-ordinates\n",
        "        retwc   = tweet.retweet_count #re-tweet count\n",
        "        try:\n",
        "            hashtag = tweet.entities[u'hashtags'][0][u'text'] #hashtags used\n",
        "        except:\n",
        "            hashtag = \"None\"\n",
        "        try:\n",
        "            rawurl = tweet.entities[u'urls'][0][u'url'] #URLs used\n",
        "            urls = url_fix(rawurl)\n",
        "        except:\n",
        "            urls    = \"None\"\n",
        "        #AUTHOR INFO\n",
        "        username  = tweet.author.name            #author/user name\n",
        "        usersince = tweet.author.created_at      #author/user profile creation date\n",
        "        followers = tweet.author.followers_count #number of author/user followers (inlink)\n",
        "        friends   = tweet.author.friends_count   #number of author/user friends (outlink)\n",
        "        authorid  = tweet.author.id              #author/user ID#\n",
        "        authorloc = tweet.author.location        #author/user location\n",
        "        #TECHNOLOGY INFO\n",
        "        geoenable = tweet.author.geo_enabled     #is author/user account geo enabled?\n",
        "        source    = tweet.source                 #platform source for tweet\n",
        "\t\t# Dongho 03/28/16\n",
        "        csvWriter.writerow([created, str(text).encode(\"utf-8\"), retwc, hashtag, followers, friends])\n",
        "        counter = counter +1\n",
        "        if (counter == c):\n",
        "            break\n",
        "\n",
        "    csvFile.close()"
      ],
      "metadata": {
        "id": "6BzyOJkrlu0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main routine\n",
        "\n",
        "def main():\n",
        "\n",
        "    global api, cmax, locords\n",
        "\n",
        "    # Geo-coordinates of five metropolitan areas\n",
        "    # London, NYC (lower, middle, upper), Wash DC, San Francisco, New Brunswick (NJ)\n",
        "    locords =  {'lo': '0, 51.503, 20km',\n",
        "                'nyl': '-74, 40.73, 2mi',\n",
        "                'nym': '-74, 40.74, 2mi',\n",
        "                'nyu': '-73.96, 40.78, 2mi',\n",
        "                'dc': '-77.04, 38.91, 2mi',\n",
        "                'sf': '-122.45, 37.74, 5km',\n",
        "                'nb': '-74.45, 40.49, 2mi'}\n",
        "    # Maximum allowed tweet count (note: Twitter sets this to ~180 per 15 minutes)\n",
        "    cmax = 50\n",
        "    # OAuth key file\n",
        "    authfile = './auth.k'\n",
        "\n",
        "    tw_parser('DontLookUp','lo','en','recent',50)\n",
        "    api = tw_oauth(authfile)\n",
        "    tw_search(api)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE4T8tp5l-4a",
        "outputId": "d0beb0d7-e1a3-4df5-a609-5c399c0839f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: DontLookUp, Location: 0, 51.503, 20km, Language: en, Search type: recent, Count: 50\n"
          ]
        }
      ]
    }
  ]
}